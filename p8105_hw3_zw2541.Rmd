---
title: "p8105_hw3_zw2541"
author: "Zixu_Wang"
date: "10/5/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(patchwork)
```

## Problem 1

#### Read and clean the data

```{r}
library(p8105.datasets)

data("brfss_smart2010")

brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, county = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent", "Very good","Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, levels = c("Excellent", "Very good","Good", "Fair", "Poor")))
```

#### Using this dataset, do or answer the following:

* In 2002, which states were observed at 7 locations?

```{r}
brfss %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  summarize(n_location = n_distinct(county)) %>% 
  filter(n_location == "7")
```

In 2002, Connecticut, Florida, and North Carolina were observed at 7 locations.

* Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

```{r}
brfss %>% 
  group_by(state,year) %>% 
  summarize(n_locations = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = n_locations, color = state)) +
  geom_line() +
  labs(
        title = "Spaghetti Plot-Number of Locations in Each State from 2002 to 2010",
        y = "Number of locations",
        x = "Year") +
  theme(legend.position = "bottom",
        legend.key.width = unit(.25, 'cm')) +
  guides(color = guide_legend(ncol = 15))
```

The figure above shows the number of locations being observed in each state from 2002 to 2010. For most states, number of locations approximately keep the same during these eight years and always below 20 locations. However, the number of locations in Florida changed dramaticly from 2006 to 2008.

* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of "Excellent" responses across locations in NY State.

```{r}
brfss %>% 
  group_by(state, year) %>% 
  filter(year %in% c("2002", "2006", "2010"),
         state == "NY",
         response == "Excellent") %>% 
  summarize(mean_prop = mean(data_value),
            sd_prop = sd(data_value)) %>% 
  knitr::kable(digits = 3)
```

The mean and sd of the proportion of "Excellent" are approximately the same in 2002, 2006 and 2010.

* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

```{r}
brfss %>% 
  group_by(year, state, response) %>% 
  summarize(response_average = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = response_average)) +
  geom_boxplot(aes(fill = factor(year))) +
  facet_grid(.~response) +
  labs(
        title = "Five-Panel Plot-Average Proportion in Each Response Category",
        y = "Average Proportion of Each Response",
        x = "Year") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Year")
```

First, we could conclude from the figures above that the mean of each response remained constent from 2002 to 2010. Second, the "Very good" response has the highest mean among this response category and the "Poor" response has the lowest. Third, the mean increases from "Poor" to "Very good", and decreases when it comes to "Excellent".

## Problem 2

#### Read and clean the data

```{r}
data(instacart)
instacart = janitor::clean_names(instacart)
skimr::skim(instacart)
```

This "instacart" dataset documents the shoppoing history of different client. From the table above, we know that there are total 1384617 observations and 15 variables. Among these 15 variables, 4 are character variables ("aisle", "department", "eval_set", "product_name"), and others are all numeric variables. The meaning of each variables are showing below:

* order_id: the id number of each order
* product_id: the id number of each product
* add_to_cart_order: the order of each product be added to the cart by a client
* reordered: whether this product has been ordered before (1 means TRUE, 0 means FALSE)
* user_id: the id number of the user
* eval_set: evaluation set
* order_number: the time of this order among the shopping history
* order_dow: the day in a week when the order has been placed(0 means Sunday, 1 means Monday and 6 means Saturday)
* order_hour_of_day: the time in a day when the order has been placed
* days_since_prior_order: the days between this order and the client's previous order
* product_name: the name of the product
* aisle_id: the id number of aisle
* department_id: the id number of department
* aisle: the name of the aisle the product belongs to
* department: the name of the department

For example, user(112108) placed an order(order_id: 1) of eight products(Bulgarian Yogurt, Organic 4% Milk Fat Whole Milk Cottage Cheese, etc) at 10am on Thursday. And the order is this user's fourth order.

#### Using this dataset, do or answer the following:

* How many aisles are there, and which aisles are the most items ordered from?

```{r}
n_distinct(instacart$aisle)

instacart %>% 
  group_by(aisle) %>% 
  summarize(n_aisle = n()) %>% 
  arrange(desc(n_aisle))
```
There are 134 distinct aisles, and "fresh vegetables" aisle is the most items ordered from.

* Make a plot that shows the number of items ordered in each aisle.

```{r, fig.height = 18}
  instacart %>% 
  group_by(aisle) %>% 
  summarize(n_aisle = n()) %>% 
  arrange(desc(n_aisle)) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle, .desc = FALSE)) %>% 
  ggplot(aes(x = aisle, y = n_aisle)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(
    title = "the Number of Items Ordered in Each Aisle",
    x = "Aisle",
    y = "Number of Items") 
```

Form the figure above, we know that the number of items in "fresh vegetables" and "fresh fruits" are much higher then any other asiles, and approximately most of the asiles about daily diet (like vegetables, fruits and drinks) is higher than others. This situation illustrates that people tend to buy daily food here. 
* Make a table showing the most popular item in each of the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits".

```{r}
instacart %>% 
  group_by(product_name, aisle) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(n_items = n()) %>% 
  group_by(aisle) %>% 
  filter(n_items == max(n_items)) %>% 
  knitr::kable()
```

The most popular item in aisle "baking ingredients" is "Light Brown Sugar", the most popular item in aisle "dog food care" is "Snack Sticks Chicken & Rice Recipe Dog Treats", and the most popular item in aisle "packaged vegetables fruits" is "Organic Baby Spinach".

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart$order_dow = recode(instacart$order_dow, "0" = "Sunday", "6" = "Saturday", "5" = "Friday", "4" = "Thursday", "3" = "Wednesday", "2" = "Tuesday", "1" = "Monday") %>% 
  factor(levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
instacart %>% 
  group_by(product_name, order_dow) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```

People tends to buy Coffee Ice Cream around 1pm to 3pm, but on Friday people may buy the cream a little bit earlier, around 12:00 at noon. And people usually buy Pink Lady Apples around noon from 11am to 2pm.

## Problem 3

#### Read and clean the data

```{r}
data(ny_noaa)
instacart = janitor::clean_names(ny_noaa)
skimr::skim(ny_noaa)
```

This "ny_noaa" dataset documents the everyday weather information from different weather station from 1.1.1981 to 12.31.2010. From the table above, we know that there are total 2595176 observations and 7 variables. The "id" variable is a characteric variable, and it represents the id number of each weather station. The "data" variable is a data variable. "prcp", "snow" and "snwd" are all numeric variables, and they indicates the precipitation, snow and snowdepth. "tmax" and "tmin" are characteric variables, they represent the maximum and minimum temperature. However, this dataset contains a lots of missing data. The "prcp" contains 145838 missing data and "snow" contains 381221 missing data, since there wasn't raining or snowing on that day. The 591786 missing data in "snwd" inllustrate that the snow on that day is not large enough to measure the depth or even no snowing on that day. There are also more than one million missing data in the "tmax" and "tmin" variables.

#### Using this dataset, do or answer the following: